{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Po1Rh3-uLO"
      },
      "source": [
        "- **Author:** Kandimalla Hemanth\n",
        "- **Gmail**: speechcodehemanth2@gmail.com\n",
        "\n",
        "#### Reinforcement Learning Essentials: A Unified Resource\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hirOnMtI936m"
      },
      "source": [
        "- Environment\n",
        "- Actions\n",
        "- steps\n",
        "- observation\n",
        "- Reward/Return\n",
        "- Value\n",
        "- policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auloV5PVAjpz"
      },
      "source": [
        " discuss the core concepts of \"Agent\" and \"Agent-Environment Interaction\" in Reinforcement Learning (RL).\n",
        "\n",
        "**1. ðŸ¤–Agent:**\n",
        "\n",
        "*   **Definition:** In Reinforcement Learning, an agent is an entity that learns to interact with an environment to achieve a specific goal. It's the decision-maker, the learner, and the actor within the RL framework. The agent takes actions, observes the environment's responses, and receives rewards, using this information to improve its behavior over time.\n",
        "*   **Key Components:**\n",
        "    *   **Policy:**  A mapping from perceived states of the environment to actions to be taken. It defines the agent's behavior. (Can be deterministic or stochastic)\n",
        "    *   **Value Function (optional but often crucial):** An estimate of how \"good\" it is to be in a particular state or to take a particular action in a state, based on future expected rewards.\n",
        "    *   **Model (optional):** A representation of the environment's dynamics. It allows the agent to predict the next state and reward given the current state and action. (Model-based vs. model-free agents)\n",
        "*   **Analogy:**  Think of an agent as a student learning a new subject. The student (agent) takes actions (studies, does exercises), observes the results (grades, understanding), and receives rewards (good grades, knowledge). The student's policy is their study strategy, their value function is their understanding of how well they're doing in each topic, and their model (if they have one) is their understanding of how the subject matter works.\n",
        "*   **Novel Analogy (for a fellow researcher):** Consider an agent as a control system that aims to regulate the behavior of a dynamical system (the environment). The agent's policy is the control law, the value function represents a cost function or performance metric, and the model (if present) is a mathematical model of the system being controlled.\n",
        "\n",
        "**2. ðŸ¤–Agent-ðŸŒŽEnvironment Interaction:**\n",
        "\n",
        "*   **Definition:** This is the fundamental loop that drives Reinforcement Learning. The agent and the environment interact sequentially over a series of discrete time steps. At each time step, the agent observes the current state of the environment, selects an action based on its policy, and the environment transitions to a new state, providing a reward to the agent. This cycle continues, and the agent's goal is to learn a policy that maximizes its cumulative reward over time.\n",
        "*   **The Interaction Loop:**\n",
        "- 1. **Environment provides State (S_t):** The environment presents the current state $S_t$ to the agent.\n",
        "- 2. **Agent takes Action (A_t):** The agent observes the state $S_t$ and selects an action $A_t$ based on its policy $\\pi(A_t | S_t)$.\n",
        "- 3. **Environment transitions and provides Reward (R_{t+1}) and next State (S_{t+1}):** The environment receives the action $A_t$, transitions to a new state $S_{t+1}$ according to its transition dynamics $P(S_{t+1} | S_t, A_t)$, and emits a reward $R_{t+1}$ to the agent based on its reward function $R(S_t, A_t, S_{t+1})$.\n",
        "- 4. **Agent receives Reward (R_{t+1}) and next State (S_{t+1}):** The agent receives the reward $R_{t+1}$ and observes the new state $S_{t+1}$.\n",
        "- 5. **Agent updates (learns):** The agent uses the experience tuple $(S_t, A_t, R_{t+1}, S_{t+1})$ to update its policy and/or value function, improving its decision-making ability.\n",
        "- 6. **Repeat:** The process repeats from step 1 with the new state $S_{t+1}$.\n",
        "*   **Mathematical Representation:** The interaction can be formalized as a Markov Decision Process (MDP) if the environment satisfies the Markov property.\n",
        "*   **Analogy:** Think of the agent-environment interaction as a conversation between two people. The environment \"speaks\" by presenting a situation (state), the agent \"responds\" with an action, and the environment \"replies\" with a consequence (reward and new state). This conversation continues, and the agent learns to \"speak\" in a way that leads to desirable \"responses\" from the environment.\n",
        "*   **Novel Analogy:**  Consider the agent-environment interaction as a feedback control loop. The environment is the system being controlled, the agent is the controller, the state is the system's output, the action is the control input, and the reward is a performance feedback signal. The agent's goal is to learn a control policy that optimizes the feedback signal over time. This is analogous to how engineers design controllers for various systems, such as robots, aircraft, or industrial processes.\n",
        "\n",
        "**Key Considerations in Agent-Environment Interaction:**\n",
        "\n",
        "*   **Exploration-Exploitation Trade-off:** The agent must balance exploring the environment to discover new information and exploiting its current knowledge to maximize rewards.\n",
        "*   **Credit Assignment Problem:** Determining which actions are responsible for the received rewards, especially when rewards are delayed.\n",
        "*   **Generalization:** The agent should be able to generalize from its experiences to unseen states and actions, allowing it to perform well in novel situations.\n",
        "*   **Partially Observable Environments:** In many real-world scenarios, the agent may not have full access to the environment's state, making the learning problem more challenging. (POMDPs)\n",
        "\n",
        "The agent-environment interaction is the core of what makes Reinforcement Learning unique and powerful. It's a dynamic process of learning through trial and error, where the agent constantly adapts its behavior based on the feedback it receives from the environment. This framework allows us to develop intelligent agents that can solve complex problems in a wide range of domains, from game playing and robotics to resource management and scientific discovery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FGdr-HmLTp1"
      },
      "source": [
        "\n",
        "\n",
        "**Reward Theory in Reinforcement Learning**\n",
        "\n",
        "*   **Definition:** Reward theory in RL is concerned with the design, interpretation, and use of reward functions. It explores how rewards shape an agent's behavior, how to design effective reward functions for complex tasks, and how different reward structures can lead to different learning outcomes. The reward function is the primary mechanism for communicating the goal of a task to the agent. It acts as a feedback signal, indicating the desirability of the agent's actions in different states.\n",
        "*   **Core Idea:** The fundamental premise is that an agent's goal is to maximize the cumulative reward it receives over time. This is often referred to as the **reward hypothesis**:\n",
        "\n",
        "    *   **Reward Hypothesis:** *All goals can be described by the maximization of expected cumulative reward.*\n",
        "\n",
        "    This hypothesis is a central assumption in RL. While it may seem simple, it has profound implications for how we design and understand intelligent agents.\n",
        "\n",
        "**Key Aspects of Reward Theory:**\n",
        "\n",
        "1. **Reward Function Design:**\n",
        "\n",
        "    *   **Definition:**  This is the process of defining a function that maps state-action-next state transitions (or sometimes just states or state-action pairs) to scalar reward values. It's a crucial and often challenging aspect of applying RL to real-world problems.\n",
        "    *   **Challenges:**\n",
        "        *   **Sparsity:** In many tasks, rewards are sparse, meaning the agent receives non-zero rewards only very occasionally (e.g., winning a game, completing a complex task). This can make learning very slow and difficult.\n",
        "        *   **Defining \"Good\" Behavior:** Translating a complex, potentially qualitative goal into a quantitative reward function that accurately reflects the desired behavior can be difficult.\n",
        "        *   **Avoiding Unintended Consequences:** A poorly designed reward function can lead to unintended behaviors, as the agent will try to maximize its reward even if it means exploiting loopholes or finding solutions that don't align with the designer's intent (often called \"reward hacking\").\n",
        "    *   **Techniques:**\n",
        "        *   **Reward Shaping:** Adding extra rewards to guide the agent towards the goal, especially in the early stages of learning. However, care must be taken to ensure that the added rewards don't change the optimal policy of the original task.\n",
        "        *   **Inverse Reinforcement Learning (IRL):**  Attempting to infer the reward function from expert demonstrations. This is useful when it's easier to demonstrate the desired behavior than to explicitly define the reward function.\n",
        "        *   **Preference-Based Learning:** Learning a reward function from qualitative feedback, such as comparisons between different trajectories.\n",
        "2. **Reward Structures:**\n",
        "\n",
        "    *   **Definition:** This refers to the different ways rewards can be distributed over time and across states. Different reward structures can significantly impact learning dynamics and the resulting policies.\n",
        "    *   **Types of Reward Structures:**\n",
        "        *   **Immediate vs. Delayed:** Immediate rewards are received directly after an action, while delayed rewards are received after a sequence of actions.\n",
        "        *   **Sparse vs. Dense:** Sparse rewards are infrequent, while dense rewards are received more frequently.\n",
        "        *   **Positive vs. Negative:** Positive rewards encourage actions, while negative rewards (penalties) discourage them.\n",
        "        *   **Stochastic vs. Deterministic:** Stochastic rewards have a probabilistic element, while deterministic rewards are fixed.\n",
        "3. **Impact of Discount Factor (Î³):**\n",
        "\n",
        "    *   **Definition:** The discount factor determines the present value of future rewards. It's a crucial parameter in the definition of the return (cumulative discounted reward).\n",
        "    *   **Impact:**\n",
        "        *   **Near-sighted vs. Far-sighted:** A smaller discount factor (closer to 0) makes the agent more myopic, prioritizing immediate rewards over long-term gains. A larger discount factor (closer to 1) makes the agent more far-sighted, valuing future rewards more heavily.\n",
        "        *   **Convergence:** The discount factor can affect the convergence properties of RL algorithms.\n",
        "4. **Theoretical Considerations:**\n",
        "\n",
        "    *   **Reward Hypothesis:** As mentioned earlier, this is a central assumption in RL. It's a powerful but also debated idea.\n",
        "    *   **Limitations of Scalar Rewards:** Some argue that complex goals may not be adequately representable by a single scalar reward function.\n",
        "    *   **Alternative Reward Frameworks:** Research is exploring alternative frameworks, such as multi-objective RL (where there are multiple reward functions to be optimized simultaneously) and hierarchical RL (where rewards are structured hierarchically to represent subgoals).\n",
        "\n",
        "**Analogies:**\n",
        "\n",
        "*   **Training a Dog:**  Think of training a dog with treats. The treats are the rewards, and the dog (agent) learns to perform actions (sit, stay, fetch) that lead to receiving treats. A sparse reward would be giving a treat only when the dog performs a complex trick perfectly. A dense reward would be giving treats for each small step towards the desired behavior.\n",
        "*   **Game Design:**  In video games, rewards are used to encourage players to engage with the game and progress. Points, power-ups, new levels, and unlocking content are all forms of rewards. A well-designed game uses rewards effectively to create a fun and engaging experience.\n",
        "*   **Economics:**  Economic systems are based on rewards (e.g., money, profits). People and companies act in ways that they believe will maximize their economic rewards.\n",
        "\n",
        "**Novel Analogy (for a fellow researcher):**\n",
        "\n",
        "*   **Reward Function as a \"Potential Energy Landscape\":** Imagine the reward function as defining a potential energy landscape over the state-action space. The agent's goal is to navigate this landscape to find regions of low potential energy (high reward). The discount factor influences the \"smoothness\" of the landscape. This analogy connects RL to concepts in physics and optimization, where systems tend to move towards states of lower potential energy.\n",
        "*   **Reward Function as a \"Communication Channel\":**  Think of the reward function as a communication channel between the designer and the agent. The designer uses the reward function to encode the desired behavior, and the agent decodes this information to learn a policy. The effectiveness of this communication depends on the clarity and expressiveness of the reward function. This analogy highlights the importance of careful reward design and the potential for miscommunication between the designer and the agent.\n",
        "\n",
        "**Implications for Building Intelligent Agents:**\n",
        "\n",
        "Reward theory is fundamental to building intelligent agents using Reinforcement Learning. Understanding how rewards shape behavior, how to design effective reward functions, and the limitations of the reward-based approach are crucial for creating agents that can solve complex problems and achieve desired goals. As we move towards more sophisticated RL applications, reward theory will continue to be a central area of research and development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRiU5-dEoIUG"
      },
      "source": [
        "\n",
        "**1. Actions (A):**\n",
        "\n",
        "*   **Definition:** Actions are the set of choices an agent can make within a given environment at each step. They influence the transition from one state to another and are fundamental to how the agent interacts with its surroundings. Actions can be discrete (e.g., move left, right, up, down) or continuous (e.g., set the angle of a steering wheel).\n",
        "*   **Mathematical Representation:**\n",
        "    *   Discrete actions:  $A = \\{a_1, a_2, ..., a_n\\}$ where each $a_i$ represents a specific action.\n",
        "    *   Continuous actions: $A \\subseteq \\mathbb{R}^n$, where $A$ is a subset of an n-dimensional real vector space, representing a range of possible actions.\n",
        "*   **Analogy:** Imagine you're playing a board game like chess. The actions are the possible moves you can make with each of your pieces. Each move represents a choice that alters the state of the board. Or in a robotics context, actions could be the torques applied to the joints of a robotic arm to achieve a desired motion.\n",
        "*   **Novel Analogy (for a fellow researcher):** Think of actions as the control signals sent to a complex dynamical system. The actions determine the trajectory of the system through its state space, similar to how control inputs guide the behavior of a satellite or a chemical reactor.\n",
        "\n",
        "**2. State (S):**\n",
        "\n",
        "*   **Definition:** A state represents a specific configuration or snapshot of the environment at a particular point in time. It encapsulates all the relevant information needed to predict the future behavior of the environment, given a particular action.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $S = \\{s_1, s_2, ..., s_m\\}$ (Discrete states)\n",
        "    *   $S \\subseteq \\mathbb{R}^m$ (Continuous states)\n",
        "*   **Analogy:** In a chess game, the state is the current arrangement of all pieces on the board. It's a complete description of the game's situation at that moment. In a self-driving car scenario, the state might include the car's position, speed, the positions of other vehicles, traffic signals, etc.\n",
        "*   **Novel Analogy:** Consider the state as a point in a high-dimensional phase space of a dynamical system. Each dimension represents a specific variable describing the system (e.g., position, velocity, temperature). The current state is a point in this space, and the system's evolution traces a trajectory through this space.\n",
        "\n",
        "**3. Environment:**\n",
        "\n",
        "*   **Definition:** The environment is the external system or context within which the agent operates. It defines the rules of the world, including how states transition based on actions and what rewards are received.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Transition Function:** $P(s' | s, a)$, which defines the probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "    *   **Reward Function:** $R(s, a, s')$, which provides the immediate reward received after transitioning from state $s$ to $s'$ by taking action $a$.\n",
        "*   **Analogy:** The environment is like the game itself (e.g., chess, a video game, or the real world for a robot). It determines how the game is played, what happens when you make a move, and what the consequences of your actions are.\n",
        "*   **Novel Analogy:** Think of the environment as an oracle or a black-box function that accepts the agent's action as input and returns the next state and the reward as output. The agent's task is to learn the internal dynamics of this oracle to maximize its cumulative reward. It's similar to system identification in control theory, where we try to understand the behavior of an unknown system by observing its inputs and outputs.\n",
        "\n",
        "**4. Observation (O):**\n",
        "\n",
        "*   **Definition:** An observation is the information that the agent receives from the environment at each step. In some cases, the observation is the same as the state (fully observable environment), but often it's a partial or noisy representation of the true state (partially observable environment).\n",
        "*   **Mathematical Representation:**\n",
        "    *   $O = \\{o_1, o_2, ..., o_k\\}$ (Discrete observations)\n",
        "    *   $O \\subseteq \\mathbb{R}^k$ (Continuous observations)\n",
        "    *   **Observation Function:** $P(o | s, a)$, which gives the probability of observing $o$ after taking action $a$ and ending up in state $s$.\n",
        "*   **Analogy:** In a poker game, your observation is the cards you hold and the cards on the table. You don't see the other players' hands (partial observability). In a robotics example, the observation might be the image from a camera, which provides a limited view of the robot's surroundings.\n",
        "*   **Novel Analogy:** Imagine the observation as a sensor reading from a complex system. The sensor might not capture all the information about the system's state, and the reading might be corrupted by noise. The agent's challenge is to infer the underlying state of the system based on these imperfect observations, akin to state estimation in filtering theory (e.g., using a Kalman filter).\n",
        "\n",
        "**5. Return (G):**\n",
        "\n",
        "*   **Definition:** The return is the cumulative, discounted reward an agent receives from a given time step until the end of an episode (or until a terminal state is reached). It represents the total reward an agent can expect to accumulate by following a particular trajectory.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
        "    *   Where:\n",
        "        *   $G_t$ is the return at time step $t$.\n",
        "        *   $R_{t+k+1}$ is the reward received at time step $t+k+1$.\n",
        "        *   $\\gamma$ is the discount factor ($0 \\leq \\gamma \\leq 1$), which determines the present value of future rewards. A smaller $\\gamma$ prioritizes immediate rewards, while a larger $\\gamma$ gives more weight to long-term rewards.\n",
        "*   **Analogy:** The return is like your total score in a game. It's not just the points you get in the current round but the sum of all points you'll accumulate until the game ends, with future points being worth slightly less than immediate points.\n",
        "*   **Novel Analogy:** Think of the return as the net present value (NPV) of a project in finance. Future rewards are discounted to reflect their reduced value compared to immediate rewards, similar to how future cash flows are discounted in NPV calculations.\n",
        "\n",
        "**6. Value (V or Q):**\n",
        "\n",
        "*   **Definition:** Value functions estimate the \"goodness\" of being in a particular state or taking a particular action in a state. They are used by the agent to make decisions and learn an optimal policy. There are two main types:\n",
        "    *   **State-Value Function (V):** Estimates the expected return starting from a given state and following a particular policy.\n",
        "    *   **Action-Value Function (Q):** Estimates the expected return starting from a given state, taking a particular action, and then following a particular policy.\n",
        "*   **Analogy:** A value function is like a map that tells you how valuable different locations are in a treasure hunt. A state-value function tells you how good a particular location is overall, while an action-value function tells you how good it is to take a specific path from that location.\n",
        "\n",
        "**7. Returns and values:**\n",
        "Returns are the foundation upon which value functions are built. Value functions, in turn, are used to estimate and predict these returns, enabling agents to make informed decisions. Returns are also used to learn the policy.\n",
        "\n",
        "**8. Policy (Ï€):**\n",
        "\n",
        "*   **Definition:** A policy defines the agent's behavior. It's a mapping from states to actions, specifying which action the agent should take in each state. Policies can be deterministic or stochastic.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Deterministic Policy:** $\\pi(s) = a$, where the policy returns a single action $a$ for each state $s$.\n",
        "    *   **Stochastic Policy:** $\\pi(a | s)$, where the policy returns a probability distribution over actions for each state $s$.\n",
        "*   **Analogy:** A policy is like a strategy in a game. It tells you what to do in every possible situation. In chess, a policy might say, \"If the opponent's queen is threatening, try to block it.\"\n",
        "*   **Novel Analogy:** Consider a policy as a control law in a feedback control system. The policy maps the current state of the system to the control action that should be applied, similar to how a PID controller determines the control signal based on the error between the desired and actual states.\n",
        "\n",
        "**9. Actual Value Function (VÏ€ or QÏ€):**\n",
        "\n",
        "*   **Definition:** The actual value function represents the true expected return an agent will achieve when starting from a particular state (or state-action pair) and following a specific policy Ï€. It's the ground truth against which estimated value functions are compared.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **State-Value Function:** $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s]$\n",
        "    *   **Action-Value Function:** $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a]$\n",
        "    *   Where $\\mathbb{E}_{\\pi}$ denotes the expected value when following policy $\\pi$.\n",
        "*   **Analogy:** The actual value function is like knowing the exact outcome of a game if you play it perfectly according to a specific strategy. It's what you'd get if you had infinite computational power and could simulate the game an infinite number of times.\n",
        "*   **Novel Analogy:**  Think of the actual value function as the solution to a differential equation that describes the evolution of the expected return over time. This solution represents the true, underlying value of a state or action under a given policy.\n",
        "\n",
        "**10. Value Function:**\n",
        "\n",
        "*   **Definition:** A value function is a function that estimates the expected return for being in a given state or taking a given action. It's a prediction of the future rewards that can be obtained from that state or action, following a specific policy. Value functions are typically learned from experience through methods like Temporal-Difference (TD) learning or Monte Carlo methods.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $\\hat{V}(s)$ or $V(s)$ (Estimated State-Value Function)\n",
        "    *   $\\hat{Q}(s, a)$ or $Q(s,a)$ (Estimated Action-Value Function)\n",
        "*   **Analogy:** The estimated value function is like your current best guess about the outcome of a game based on your experience and understanding of the rules. It might not be perfect, but it's what you use to guide your decisions.\n",
        "*   **Novel Analogy:** Consider the estimated value function as an approximation of the true value function, similar to how we use Taylor series to approximate complex functions. The estimated value function is a model that tries to capture the essential features of the true value function, allowing the agent to generalize to unseen states and actions.\n",
        "\n",
        "\n",
        "\n",
        "# lecture 2\n",
        "\n",
        "**1. Values (V or Q):**\n",
        "\n",
        "*   **Definition:** Value functions estimate the \"goodness\" of being in a particular state or taking a particular action in a state. They are crucial for decision-making in RL as they quantify the long-term desirability of states and actions.\n",
        "    *   **State-Value Function (V):** Estimates the expected return starting from a given state and following a particular policy.\n",
        "    *   **Action-Value Function (Q):** Estimates the expected return starting from a given state, taking a particular action, and then following a particular policy.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **State-Value Function:**  $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]$, where $G_t$ is the return from time step $t$ under policy $\\pi$.\n",
        "    *   **Action-Value Function:** $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]$, where $G_t$ is the return from time step $t$ after taking action $a$ in state $s$ and following policy $\\pi$.\n",
        "*   **Analogy:** Imagine you're planning a road trip. The state-value function would be like having a map that tells you the overall desirability of each city (e.g., \"This city has great attractions and restaurants\"). The action-value function is more specific, like knowing the value of taking a particular highway from a city (e.g., \"This highway is scenic but has tolls\").\n",
        "*   **Novel Analogy (for a fellow researcher):** Think of value functions as potential energy landscapes in physics. The state space is like a terrain, and the value function represents the potential energy at each point. The agent seeks to navigate this landscape to find low-potential-energy regions (high-value states), similar to how a ball rolls downhill to minimize its potential energy.\n",
        "\n",
        "**2. Regret, Counting Regret:**\n",
        "\n",
        "*   **Definition:** Regret quantifies the performance loss incurred by an agent for not choosing the optimal action at each time step. It measures the difference between the reward obtained by the agent and the reward that could have been obtained by always choosing the best action in hindsight. Counting regret simply means that instead of weighting the terms by the discount factor, we weigh all terms equally.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Regret at time step T:** $R_T = \\max_{a \\in A} \\sum_{t=1}^T r_t(a) - \\sum_{t=1}^T r_t(a_t)$, where $r_t(a)$ is the reward received for action $a$ at time $t$, and $a_t$ is the action chosen by the agent at time $t$.\n",
        "    *   **Cumulative Regret up to time T:**  $\\text{Regret}(T) = \\sum_{t=1}^{T} R_t$\n",
        "    *   **Average Regret:** $\\frac{1}{T}\\sum_{t=1}^T r_t(a^*)-r_t(a_t)$, where $a^*$ is the best action.\n",
        "    *   **Counting Regret**: $\\sum_{t=1}^T r_t(a^*)-r_t(a_t)$, where $a^*$ is the best action.\n",
        "\n",
        "*   **Analogy:** Imagine you're investing in stocks. Regret would be the difference between your actual profit and the profit you could have made if you had always invested in the best-performing stock.\n",
        "*   **Novel Analogy:**  Think of regret as the \"opportunity cost\" in economics. It represents the value of the best foregone alternative. In RL, the agent is constantly making decisions, and regret measures the cost of not making the optimal decisions. In multi-armed bandit problems, algorithms are often designed to minimize cumulative regret, ensuring that the agent learns to exploit the best option quickly while still exploring other potentially good options.\n",
        "\n",
        "**3. Policy Search:**\n",
        "\n",
        "*   **Definition:** Policy search methods directly search for an optimal policy within a predefined policy space, rather than indirectly deriving it from a value function. They often represent policies using parameterized functions (e.g., neural networks) and optimize the parameters to maximize the expected return.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Policy is often parameterized: $\\pi(a|s; \\theta)$, where $\\theta$ represents the policy parameters.\n",
        "    *   Objective is to find $\\theta$ that maximizes the expected return: $\\theta^* = \\arg\\max_{\\theta} J(\\theta)$, where $J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[G_0]$ is the expected return under policy $\\pi_{\\theta}$.\n",
        "*   **Analogy:** Imagine you're trying to train a dog to perform a trick. Instead of teaching the dog the value of each intermediate step, you directly teach it the sequence of actions that lead to the desired outcome. You adjust the dog's behavior until it consistently performs the trick correctly.\n",
        "*   **Novel Analogy:** Think of policy search as tuning the parameters of a complex control system to achieve a desired behavior. Instead of designing a controller based on a detailed model of the system, you directly adjust the controller's parameters (e.g., gains, thresholds) to optimize its performance based on observed outcomes.\n",
        "\n",
        "**4. Gradient Bandits:**\n",
        "\n",
        "*   **Definition:** Gradient bandit algorithms are a type of policy search method specifically designed for multi-armed bandit problems. They update action preferences based on the estimated gradient of the expected reward with respect to those preferences.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Action preferences: $H_t(a)$ for each action $a$.\n",
        "    *   Policy (softmax): $\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b} e^{H_t(b)}}$\n",
        "    *   Preference update rule: $H_{t+1}(a) = H_t(a) + \\alpha (R_t - \\bar{R}_t)(I_{a=A_t} - \\pi_t(a))$, where $\\alpha$ is a step-size parameter, $R_t$ is the reward at time $t$, $\\bar{R}_t$ is an average of past rewards, and $I_{a=A_t}$ is an indicator function that is 1 if $a = A_t$ and 0 otherwise.\n",
        "*   **Analogy:** Imagine you're conducting A/B testing on a website to find the best headline. Gradient bandits would be like incrementally adjusting the wording of the headline based on the click-through rate, moving towards the wording that yields the highest engagement.\n",
        "*   **Novel Analogy:** Think of gradient bandits as a form of stochastic gradient ascent in the space of action preferences. The algorithm takes small steps in the direction that is estimated to increase the expected reward, similar to how gradient ascent is used to find the maximum of a function.\n",
        "\n",
        "**5. Upper Confidence Bounds (UCB):**\n",
        "\n",
        "*   **Definition:** UCB is a popular algorithm for balancing exploration and exploitation in multi-armed bandit problems. It selects actions based on an upper confidence bound for their expected reward, which incorporates both the estimated value of the action and the uncertainty associated with that estimate.\n",
        "*   **Mathematical Representation:**\n",
        "    *   UCB action selection: $A_t = \\arg\\max_{a} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$, where $Q_t(a)$ is the estimated value of action $a$ at time $t$, $N_t(a)$ is the number of times action $a$ has been selected up to time $t$, $c$ is a constant that controls the exploration-exploitation trade-off, and $ln$ is natural logarithm.\n",
        "*   **Analogy:** Imagine you're choosing a restaurant to dine at. UCB would be like choosing a restaurant based not only on its average rating but also on how many reviews it has. A restaurant with a slightly lower average rating but fewer reviews might be selected because it has a higher potential to be better than its current rating suggests.\n",
        "*   **Novel Analogy:** Think of UCB as a form of \"optimism in the face of uncertainty.\" The algorithm prefers actions that are either known to be good (high estimated value) or highly uncertain (large confidence bound), encouraging exploration of potentially promising options. It's similar to how scientists design experiments to test hypotheses that have a high potential to yield significant results, even if the current evidence is limited.\n",
        "\n",
        "**6. Bayesian Bandits:**\n",
        "\n",
        "*   **Definition:** Bayesian bandits maintain a posterior distribution over the expected reward for each action. They update these distributions based on observed rewards using Bayes' theorem, allowing for a more principled approach to exploration and exploitation.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Prior distribution over expected rewards: $P(\\mu_a)$ for each action $a$, where $\\mu_a$ is the true mean reward of action $a$.\n",
        "    *   Likelihood function: $P(r | \\mu_a)$ for reward $r$ given $\\mu_a$.\n",
        "    *   Posterior distribution update (Bayes' theorem): $P(\\mu_a | r) = \\frac{P(r | \\mu_a) P(\\mu_a)}{\\int P(r | \\mu) P(\\mu) d\\mu}$\n",
        "*   **Analogy:** Imagine you're a doctor trying to determine the effectiveness of different treatments for a disease. Bayesian bandits would be like starting with a prior belief about each treatment's success rate and updating those beliefs as you observe the outcomes of patients treated with each method.\n",
        "*   **Novel Analogy:** Think of Bayesian bandits as a form of \"scientific inference\" in the context of decision-making. The agent starts with a set of hypotheses (prior distributions) about the effectiveness of each action and updates those hypotheses based on experimental evidence (observed rewards), similar to how scientists refine their theories based on experimental data.\n",
        "\n",
        "**7. Probability Matching:**\n",
        "\n",
        "*   **Definition:** Probability matching is a strategy where the agent chooses an action with a probability that matches the estimated probability that the action is optimal. It's a simple heuristic that can be effective in some bandit settings.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Action selection probability: $\\pi_t(a) = P(a \\text{ is optimal at time } t)$, where the probability is estimated based on the current knowledge about the actions.\n",
        "*   **Analogy:** Imagine you're betting on a horse race. Probability matching would be like placing bets on each horse in proportion to your estimated probability of each horse winning the race.\n",
        "*   **Novel Analogy:** Think of probability matching as a form of \"diversification\" in decision-making. Instead of always choosing the action that currently appears to be the best, the agent spreads its choices across multiple actions, reflecting its uncertainty about which action is truly optimal. It's similar to how investors diversify their portfolios to mitigate risk.\n",
        "\n",
        "**8. Thompson Sampling:**\n",
        "\n",
        "*   **Definition:** Thompson Sampling is a specific type of probability matching algorithm for Bayesian bandits. It selects actions by sampling from the posterior distribution over expected rewards and choosing the action with the highest sampled value.\n",
        "*   **Mathematical Representation:**\n",
        "    *   At each time step $t$:\n",
        "        1. Sample a value $\\theta_a$ from the posterior distribution $P(\\mu_a | \\text{data})$ for each action $a$.\n",
        "        2. Select the action with the highest sampled value: $A_t = \\arg\\max_a \\theta_a$.\n",
        "*   **Analogy:** Imagine you're a talent scout trying to select the best musician from a group of candidates. Thompson Sampling would be like having each musician perform a short piece, then selecting the musician who gave the best performance in that particular instance, even if another musician has a higher average rating overall.\n",
        "*   **Novel Analogy:** Think of Thompson Sampling as a form of \"stochastic simulation\" for decision-making under uncertainty. The agent simulates possible outcomes for each action based on its current knowledge and chooses the action that performs best in the simulation. It's similar to how Monte Carlo methods are used to estimate the value of complex systems by simulating their behavior many times.\n",
        "\n",
        "**9. Information State Space:**\n",
        "\n",
        "*   **Definition:** In partially observable environments, the agent may not have access to the true state of the environment. The information state space is a space of probability distributions over the underlying states, representing the agent's belief about the current state given its past observations and actions.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Information state: $b_t(s) = P(S_t = s | O_1, A_1, ..., O_{t-1}, A_{t-1}, O_t)$, which is the probability that the true state is $s$ at time $t$ given the history of observations and actions.\n",
        "    *   The information state space is the set of all possible belief states: $\\mathcal{B} = \\{b | b(s) \\geq 0, \\sum_{s \\in S} b(s) = 1\\}$.\n",
        "*   **Analogy:** Imagine you're playing a game of Clue (Cluedo). You don't know the true solution (who, where, what weapon), but you gather clues over time. Your information state is your current belief about the likelihood of each possible solution based on the clues you've gathered.\n",
        "*   **Novel Analogy:** Think of the information state space as a \"belief space\" in a Bayesian inference problem. Each point in this space represents a different hypothesis about the true state of the world, and the agent's task is to navigate this space to find the hypothesis that best explains its observations. It's similar to how Bayesian filters (e.g., Kalman filters) track the probability distribution over the state of a dynamical system based on noisy measurements.\n",
        "\n",
        "\n",
        "# lecture 3\n",
        "\n",
        "\n",
        "**1. Markov Decision Process (MDP):**\n",
        "\n",
        "*   **Definition:** An MDP is a mathematical framework for modeling sequential decision-making problems where outcomes are partly random and partly under the control of a decision-maker (the agent). It provides a formal way to describe the interaction between an agent and its environment in terms of states, actions, transitions, and rewards.\n",
        "*   **Mathematical Representation:** An MDP is defined by a tuple $(S, A, P, R, \\gamma)$, where:\n",
        "    *   $S$ is a set of states.\n",
        "    *   $A$ is a set of actions.\n",
        "    *   $P$ is a state transition probability function: $P(s' | s, a) = \\mathbb{P}(S_{t+1} = s' | S_t = s, A_t = a)$, representing the probability of transitioning to state $s'$ from state $s$ after taking action $a$.\n",
        "    *   $R$ is a reward function: $R(s, a, s')$ is the immediate reward received after transitioning from state $s$ to $s'$ by taking action $a$. (Sometimes it is simplified as $R(s,a)$ or $R(s)$.)\n",
        "    *   $\\gamma$ is a discount factor ($0 \\leq \\gamma \\leq 1$) that determines the present value of future rewards.\n",
        "*   **Analogy:** Think of an MDP as a board game like chess or checkers, but with an element of chance (like rolling dice). The states are the board configurations, the actions are the possible moves, the transition probabilities are determined by the rules of the game and the dice rolls, the rewards are points earned or penalties incurred, and the discount factor reflects the fact that winning sooner is better than winning later.\n",
        "*   **Novel Analogy (for a fellow researcher):**  Consider an MDP as a discrete-time stochastic control problem. The agent's goal is to find a control policy that optimizes the expected cumulative reward over time, taking into account the probabilistic nature of the system's dynamics. This is analogous to designing a controller for a robot or a chemical process, where the system's behavior is influenced by both the control inputs and random disturbances.\n",
        "\n",
        "**2. Markov Property:**\n",
        "\n",
        "*   **Definition:** The Markov property states that the future is independent of the past given the present. In the context of MDPs, it means that the probability of transitioning to the next state and receiving a particular reward depends only on the current state and action, not on the history of previous states and actions.\n",
        "*   **Mathematical Representation:** $\\mathbb{P}(S_{t+1} = s', R_{t+1} = r | S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0) = \\mathbb{P}(S_{t+1} = s', R_{t+1} = r | S_t, A_t)$ for all $s', r, s_t, a_t$ and all possible histories.\n",
        "*   **Analogy:** Imagine you're flipping a fair coin. The probability of getting heads on the next flip is always 50%, regardless of whether you got heads or tails on the previous flips. The coin has no memory of its past outcomes.\n",
        "*   **Novel Analogy:** Think of the Markov property as a \"sufficient statistic\" condition. The current state encapsulates all the information relevant for predicting the future, making the history of the process irrelevant. This is similar to how a sufficient statistic in statistical inference captures all the information about a parameter contained in a sample.\n",
        "\n",
        "**3. Returns Types:**\n",
        "\n",
        "*   **Definition:** The return is the cumulative, discounted reward an agent receives from a given time step onward. Different types of returns are used depending on the nature of the task (episodic or continuing).\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Finite-horizon (Episodic) Return:** $G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t-1} R_T$, where $T$ is the final time step of an episode.\n",
        "    *   **Infinite-horizon (Continuing) Return:** $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
        "    *   **Average Reward (Continuing):** $\\rho^{\\pi} = \\lim_{n\\to\\infty} \\frac{1}{n} \\mathbb{E}[R_1 + R_2 + ... + R_n]$, used when discounting is not appropriate.\n",
        "*   **Analogy:** In a game, the finite-horizon return is like your total score at the end of the game. The infinite-horizon return is like your average score per turn if the game were to continue indefinitely, with future scores discounted. The average reward is also for infinite games, but your average score per turn without considering future rounds being less valuable.\n",
        "*   **Novel Analogy:** Think of returns as different ways of aggregating the value of a stream of rewards over time. The finite-horizon return is like the sum of a series up to a certain point, the infinite-horizon return is like the sum of an infinite series with a convergence factor (discount factor), and the average reward is like the time average of a signal in signal processing.\n",
        "\n",
        "**4. Bellman Equations:**\n",
        "\n",
        "*   **Definition:** The Bellman equations are a set of recursive equations that express the value of a state (or state-action pair) in terms of the expected values of its successor states (or state-action pairs) and the immediate rewards received. They are fundamental to understanding and solving MDPs.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Bellman Equation for** $V^{\\pi}$: $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t = s] = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^{\\pi}(s')]$\n",
        "    *   **Bellman Equation for** $Q^{\\pi}$: $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a')]$\n",
        "    *   **Bellman Optimality Equation for** $V^*$: $V^*(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$\n",
        "    *   **Bellman Optimality Equation for** $Q^*$: $Q^*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'} Q^*(s', a')]$\n",
        "\n",
        "*   **Analogy:** Imagine you're navigating a maze. The Bellman equation for the value of a location in the maze is like saying, \"The value of this location is the immediate reward I get here plus the discounted value of the best neighboring location I can move to.\"\n",
        "*   **Novel Analogy:** Think of the Bellman equations as a \"self-consistency\" condition for value functions. They state that the value of a state (or state-action pair) must be equal to the expected immediate reward plus the discounted expected value of the next state (or state-action pair) under the given policy. This is similar to how fixed-point equations in mathematics define a solution that is consistent with itself.\n",
        "\n",
        "**5. Bellman Equation in Matrix Form:**\n",
        "\n",
        "*   **Definition:** For finite MDPs, the Bellman equation for the state-value function can be expressed in a compact matrix form, which is useful for computational purposes.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$, where:\n",
        "        *   $V^{\\pi}$ is a column vector of state values.\n",
        "        *   $R^{\\pi}$ is a column vector of expected immediate rewards under policy $\\pi$: $R^{\\pi}_s = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)r$\n",
        "        *   $P^{\\pi}$ is a state transition probability matrix under policy $\\pi$: $P^{\\pi}_{ss'} = \\sum_a \\pi(a|s) p(s'|s,a)$\n",
        "    *   This can be solved directly as: $V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$\n",
        "*   **Analogy:**  This is similar to solving a system of linear equations, where the value function is the unknown vector, and the rewards and transition probabilities define the coefficients of the equations.\n",
        "*   **Novel Analogy:** Think of the matrix form of the Bellman equation as representing a linear dynamical system. The value function is the state of the system, the reward vector is the input, and the transition matrix determines the system's dynamics. Solving the equation is like finding the steady-state response of the system to the given input.\n",
        "\n",
        "**6. Policy Evaluation:**\n",
        "\n",
        "*   **Definition:** Policy evaluation is the process of computing the state-value function $V^{\\pi}$ (or the action-value function $Q^{\\pi}$) for a given policy $\\pi$. It answers the question, \"How good is this policy?\"\n",
        "*   **Mathematical Representation:**\n",
        "    *   Iterative Policy Evaluation: Start with an arbitrary initial value function $V_0$. Then, iteratively update the value function using the Bellman equation until convergence:\n",
        "        *   $V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_k(s')]$ for all $s \\in S$\n",
        "*   **Analogy:** Imagine you have a strategy for playing a game. Policy evaluation is like simulating the game many times using that strategy to see what your average score would be.\n",
        "*   **Novel Analogy:** Think of policy evaluation as computing the \"response\" of a system to a particular input. The policy is the input, the value function is the response, and the Bellman equation defines the system's dynamics. Iterative policy evaluation is like simulating the system's behavior over time until it reaches a steady state.\n",
        "\n",
        "**7. Policy Improvement:**\n",
        "\n",
        "*   **Definition:** Policy improvement is the process of creating a new policy $\\pi'$ that is better than or equal to a given policy $\\pi$, based on the value function $V^{\\pi}$ or $Q^{\\pi}$.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **Greedy Policy Improvement:**  $\\pi'(s) = \\arg\\max_a Q^{\\pi}(s, a)$ for all $s \\in S$. This creates a new policy that chooses the action with the highest action-value in each state, according to the current action-value function.\n",
        "    *   **Policy Improvement Theorem:** If $\\pi'$ is obtained from $\\pi$ by greedy policy improvement, then $V^{\\pi'}(s) \\geq V^{\\pi}(s)$ for all $s \\in S$.\n",
        "*   **Analogy:** Imagine you have a strategy for playing a game, and you've evaluated how good it is. Policy improvement is like looking at your strategy and saying, \"In this situation, I should have done this other thing instead because it would have led to a better outcome.\" You then update your strategy accordingly.\n",
        "*   **Novel Analogy:** Think of policy improvement as a \"gradient ascent\" step in policy space. By greedily choosing actions based on the current value function, the agent takes a step in the direction that is estimated to increase the expected return. This is similar to how gradient ascent is used to find the maximum of a function by iteratively moving in the direction of the gradient.\n",
        "\n",
        "**8. Value Iteration:**\n",
        "\n",
        "*   **Definition:** Value iteration is an algorithm that combines policy evaluation and policy improvement to find the optimal value function $V^*$ and an optimal policy $\\pi^*$. It iteratively applies the Bellman optimality equation until convergence.\n",
        "*   **Mathematical Representation:**\n",
        "    *   Start with an arbitrary initial value function $V_0$.\n",
        "    *   Iteratively update the value function using the Bellman optimality equation:\n",
        "        *   $V_{k+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_k(s')]$ for all $s \\in S$\n",
        "    *   The optimal policy can then be derived: $\\pi^*(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$\n",
        "*   **Analogy:** Imagine you're trying to find the shortest path through a maze. Value iteration is like starting with an estimate of the distance from each location to the exit, then iteratively updating those estimates by considering the distances of neighboring locations. Eventually, the estimates will converge to the true shortest distances, and you can find the shortest path by always moving to the neighbor with the smallest estimated distance to the exit.\n",
        "*   **Novel Analogy:** Think of value iteration as a \"dynamic programming\" approach to solving the Bellman optimality equation. The algorithm breaks down the problem of finding the optimal value function into smaller subproblems (finding the optimal value of each state) and uses the solutions to these subproblems to construct the solution to the overall problem. This is similar to how dynamic programming is used to solve other optimization problems, such as finding the shortest path in a graph or the optimal sequence alignment in bioinformatics.\n",
        "\n",
        "# lecture 4\n",
        "\n",
        "**1. Bellman Expectation Equations:**\n",
        "\n",
        "*   **Definition:** The Bellman expectation equations express the value of a state (or state-action pair) under a given policy as the expected immediate reward plus the discounted expected value of the next state (or state-action pair) under that policy. They are recursive equations that define the value function in terms of itself.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **For the state-value function** ($V^{\\pi}$):\n",
        "        *   $V^{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t = s]$\n",
        "        *   $V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^{\\pi}(s')]$\n",
        "    *   **For the action-value function** ($Q^{\\pi}$):\n",
        "        *   $Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$\n",
        "        *   $Q^{\\pi}(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a')]$\n",
        "*   **Analogy:** Imagine you're following a set of instructions (a policy) to bake a cake. The Bellman expectation equation for the value of being in a particular stage of the process (e.g., \"batter mixed\") is like saying, \"The value of this stage is the immediate reward I get here (e.g., the satisfaction of having mixed the batter) plus the discounted expected value of the next stage (e.g., \"batter in the oven\") if I continue to follow the instructions.\"\n",
        "*   **Novel Analogy (for a fellow researcher):** Think of the Bellman expectation equations as defining a \"recursive relationship\" between the values of different states (or state-action pairs) under a given policy. This recursive structure is similar to how recursive functions are defined in computer science or how recursive sequences are defined in mathematics.\n",
        "\n",
        "**2. Bellman Optimality Equations:**\n",
        "\n",
        "*   **Definition:** The Bellman optimality equations express the optimal value of a state (or state-action pair) as the maximum expected immediate reward plus the discounted expected optimal value of the next state (or state-action pair), taken over all possible actions. They define the optimal value function in terms of itself.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **For the optimal state-value function** ($V^*$):\n",
        "        *   $V^*(s) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t = s, A_t = a]$\n",
        "        *   $V^*(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^*(s')]$\n",
        "    *   **For the optimal action-value function** ($Q^*$):\n",
        "        *   $Q^*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') | S_t = s, A_t = a]$\n",
        "        *   $Q^*(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'} Q^*(s', a')]$\n",
        "*   **Analogy:** Imagine you're trying to find the shortest route on a map. The Bellman optimality equation for the optimal value of a location is like saying, \"The shortest distance from here to the destination is the minimum of the distances I can get by taking one step in any direction and then following the shortest path from that new location to the destination.\"\n",
        "*   **Novel Analogy:** Think of the Bellman optimality equations as defining a \"fixed-point\" of an optimization problem. The optimal value function is a fixed point because it satisfies the equation â€“ the value of each state (or state-action pair) is equal to the maximum expected return that can be obtained from that state (or state-action pair), which is consistent with the definition of optimality.\n",
        "\n",
        "**3. Bellman Optimality Operator:**\n",
        "\n",
        "*   **Definition:** The Bellman optimality operator ($\\mathcal{T}$) is an operator that transforms a value function into a new value function by applying the right-hand side of the Bellman optimality equation. It's a key concept for understanding value iteration.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **For state-value functions:** $(\\mathcal{T}V)(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
        "    *   **For action-value functions:** $(\\mathcal{T}Q)(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\max_{a'} Q(s', a')]$\n",
        "*   **Analogy:** Imagine you have a machine that takes a map of estimated distances to a destination as input and outputs a new map where the estimated distance for each location is updated based on the shortest paths through its neighbors. The Bellman optimality operator is like this machine.\n",
        "*   **Novel Analogy:** Think of the Bellman optimality operator as a \"nonlinear transformation\" in the space of value functions. It maps one value function to another, and the fixed point of this transformation is the optimal value function. This is similar to how linear operators are used to transform vectors in linear algebra.\n",
        "\n",
        "**4. Properties of the Bellman Operator:**\n",
        "\n",
        "*   **Definition:** The Bellman operator has several important properties that make it useful for solving MDPs.\n",
        "*   **Properties:**\n",
        "    *   **Monotonicity:** If $V_1(s) \\leq V_2(s)$ for all $s$, then $(\\mathcal{T}V_1)(s) \\leq (\\mathcal{T}V_2)(s)$ for all $s$. This means that if one value function is always greater than or equal to another, then applying the Bellman operator will preserve that relationship.\n",
        "    *   **Contraction:** The Bellman operator is a $\\gamma$-contraction mapping with respect to the maximum norm: $||\\mathcal{T}V_1 - \\mathcal{T}V_2||_{\\infty} \\leq \\gamma ||V_1 - V_2||_{\\infty}$. This means that applying the Bellman operator brings value functions closer together, and it guarantees the existence of a unique fixed point.\n",
        "*   **Analogy:**  The contraction property is like having a rubber band that is stretched between two points. Each time you apply the Bellman operator, it's like letting the rubber band contract a little bit. Eventually, the rubber band will reach its resting length, which corresponds to the fixed point of the operator.\n",
        "*   **Novel Analogy:** Think of the Bellman operator as a \"convergent iterative process\" in the space of value functions. The contraction property ensures that repeated application of the operator will converge to a unique solution, similar to how the iterative methods for solving linear equations (e.g., Jacobi, Gauss-Seidel) converge to the solution.\n",
        "\n",
        "**5. Value Iteration through the Lens of the Bellman Operator:**\n",
        "\n",
        "*   **Definition:** Value iteration can be viewed as repeatedly applying the Bellman optimality operator to an initial value function until it converges to the optimal value function.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $V_{k+1} = \\mathcal{T}V_k$, which means $V_{k+1}(s) = (\\mathcal{T}V_k)(s) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_k(s')]$ for all $s$.\n",
        "    *   Due to the contraction property, the sequence $\\{V_k\\}$ converges to the optimal value function $V^*$.\n",
        "*   **Analogy:**  Imagine you're repeatedly refining a sculpture. Each application of the Bellman operator is like using a chisel to remove a bit more material, getting closer to the desired shape (the optimal value function).\n",
        "*   **Novel Analogy:** Think of value iteration as a \"fixed-point iteration\" for finding the optimal value function. The Bellman optimality operator is the function being iterated, and the fixed point is the optimal value function. This is similar to how other fixed-point iteration methods (e.g., Newton's method) are used to find the roots of equations.\n",
        "\n",
        "**6. Bellman Expectation Operator:**\n",
        "\n",
        "*   **Definition:** The Bellman expectation operator ($\\mathcal{T}^{\\pi}$) is an operator that transforms a value function into a new value function by applying the right-hand side of the Bellman expectation equation for a given policy $\\pi$.\n",
        "*   **Mathematical Representation:**\n",
        "    *   **For state-value functions:** $(\\mathcal{T}^{\\pi}V)(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
        "    *   **For action-value functions:** $(\\mathcal{T}^{\\pi}Q)(s, a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a')]$\n",
        "*   **Analogy:** Imagine you have a machine that takes a map of estimated values for locations under a particular set of travel directions (a policy) and outputs a new map where the estimated value for each location is updated based on following those directions to its neighbors. The Bellman expectation operator is like this machine.\n",
        "*   **Novel Analogy:** Think of the Bellman expectation operator as a \"linear transformation\" in the space of value functions, parameterized by the policy $\\pi$. It maps one value function to another, and the fixed point of this transformation is the value function for the given policy.\n",
        "\n",
        "**7. Properties of the Bellman Expectation Operator:**\n",
        "\n",
        "*   The Bellman expectation operator also exhibits:\n",
        "    *   **Monotonicity:** If $V_1(s) \\leq V_2(s)$ for all $s$, then $(\\mathcal{T}^{\\pi}V_1)(s) \\leq (\\mathcal{T}^{\\pi}V_2)(s)$ for all $s$.\n",
        "    *   **Contraction:** It is a $\\gamma$-contraction mapping with respect to the maximum norm: $||\\mathcal{T}^{\\pi}V_1 - \\mathcal{T}^{\\pi}V_2||_{\\infty} \\leq \\gamma ||V_1 - V_2||_{\\infty}$.\n",
        "*   The contraction property is crucial because it guarantees that iterative policy evaluation converges to a unique solution.\n",
        "\n",
        "**8. Policy Evaluation (Revisited through the Lens of the Bellman Expectation Operator):**\n",
        "\n",
        "*   **Definition:** Policy evaluation can be viewed as repeatedly applying the Bellman expectation operator for a given policy $\\pi$ to an initial value function until it converges to the true value function for that policy.\n",
        "*   **Mathematical Representation:**\n",
        "    *   $V_{k+1} = \\mathcal{T}^{\\pi}V_k$, which means $V_{k+1}(s) = (\\mathcal{T}^{\\pi}V_k)(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_k(s')]$\n",
        "    *   Due to the contraction property, the sequence $\\{V_k\\}$ converges to the value function $V^{\\pi}$ for the policy $\\pi$.\n",
        "*   **Analogy:** Imagine you're repeatedly playing a game using a fixed strategy and updating your estimate of your average score after each game. Each application of the Bellman expectation operator is like playing one more game and updating your estimate.\n",
        "*   **Novel Analogy:** Think of policy evaluation as a \"power iteration\" method for finding the dominant eigenvector of a matrix (in this case, the matrix representation of the Bellman expectation operator). The value function is the eigenvector, and the repeated application of the operator converges to this eigenvector, which corresponds to the value function for the given policy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F9XxtR4oR0k"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Core Distinction: The Presence of an Environment Model**\n",
        "\n",
        "The fundamental difference between these two types of RL agents lies in whether they explicitly learn and utilize a *model* of the environment.\n",
        "\n",
        "-   **Environment Model:** A model, in this context, is a representation of how the environment works. It allows the agent to predict:\n",
        "\n",
        "    -  **Transitions:** How the environment's state changes in response to the agent's actions. Mathematically, this is represented by the transition function:\n",
        "        `T(s', r | s, a)  or P(s' | s, a) and R(s, a)` which represents the probability of transitioning to state `s'` and receiving reward `r` after taking action `a` in state `s`. Sometimes the reward is simplified as `R(s,a)` or `R(s,s')`.\n",
        "    -   **Rewards:** The immediate feedback the agent receives from the environment after taking an action.\n",
        "\n",
        "**Model-Based Reinforcement Learning: The Architect**\n",
        "\n",
        "-   **Definition:** Model-based agents explicitly learn a model of the environment (or have one provided). They then use this model to plan or predict future outcomes, choosing actions that maximize expected long-term rewards. They use the model for simulation.\n",
        "-   **Novel Analog:** **A Civil Engineer Designing a Bridge**\n",
        "    -   **Model:** The engineer has a deep understanding of physics, material properties, and structural mechanics (the environment model).\n",
        "    -   **Planning:** Before building the bridge, the engineer uses the model to simulate and analyze the structure under different conditions (load, stress, etc.) to find an optimal design. They consider future outcomes and choose the optimal design (actions) based on the simulated outcome. This is essentially the bridge's \"environment\" for the structural integrity.\n",
        "    -   Just like the engineer cannot start building without having a good understanding of material properties, an agent cannot just act in a model-based way if they do not have the knowledge of how the environment will react based on their actions.\n",
        "\n",
        "-   **Mathematical Framework:**\n",
        "    1.  **Model Learning (If the model is unknown):**\n",
        "        -   The agent interacts with the environment, collecting data tuples of the form: `(s_t, a_t, r_t, s_{t+1})`.\n",
        "        -   This data is used to estimate the transition function `T(s', r | s, a)` using techniques from supervised learning (regression or classification).\n",
        "        -   For example, `P(s' | s, a)` can be estimated by counting the number of times we transition to `s'` when taking action `a` in state `s`, divided by the total number of times we were in `s` and took `a`.\n",
        "            `P(s' | s, a) = Count(s, a, s') / Count(s, a)`\n",
        "            and `R(s,a,s') = mean of reward we get when we go from s, to s' via action a`\n",
        "    2.  **Planning:**\n",
        "        -   The agent utilizes the learned model to plan. Common methods include:\n",
        "            -   **Dynamic Programming (Value/Policy Iteration):** When the model is known and the state and action spaces are discrete, we can calculate the value function of each state by recursively using the Bellman equation.\n",
        "            \n",
        "            -  **Bellman Optimality Equation (for Value Iteration):**\n",
        "                    `V*(s) = max_a [ Î£_{s'} P(s' | s, a) * (R(s,a,s') + Î³ * V*(s')) ]`\n",
        "\n",
        "            Where:\n",
        "            -  `V*(s)` is the optimal value function for state s.\n",
        "            -  `a` represents actions available in state `s`.\n",
        "            -  `P(s' | s, a)` is the transition probability from state s to s' when taking action `a`.\n",
        "            -  `R(s,a,s')` is the reward received for transitioning from `s` to `s'` by taking acti`a`.\n",
        "            -   `Î³` is the discount factor, which controls how much importance is given to future rewards.\n",
        "                -   **Policy Iteration:** Alternates between policy evaluation (calculating the value function for the current policy) and policy improvement (updating the policy to greedily select actions based on the value function)\n",
        "\n",
        "            -   **Monte Carlo Tree Search (MCTS):** Commonly used when the model is known but the state space is large (e.g., in games like Go). MCTS simulates trajectories by sampling actions from the model.\n",
        "-   **Procedures:**\n",
        "    1.  **Initialization:** Initialize the model (either randomly or with prior knowledge)\n",
        "    2.  **Data Collection:** Interact with the environment, storing experience tuples: `(s_t, a_t, r_t, s_{t+1})`.\n",
        "    3.  **Model Update:** Update the model based on the new experience data.\n",
        "    4.  **Planning:** Use the updated model and planning algorithms to compute the optimal policy/value function.\n",
        "    5.  **Action Selection:** Select actions based on the planned optimal policy (e.g., take the action that maximizes the value from state `s`).\n",
        "    6.  **Repeat:** Repeat the process, continuously refine the model and policy.\n",
        "\n",
        "-   **Advantages:**\n",
        "    *   **Sample Efficiency:** Can learn effective strategies with fewer interactions because they can \"imagine\" different possibilities through simulations.\n",
        "    *   **Generalization:** A good model can generalize to unseen states or environments.\n",
        "    *   **Safety:** Planning allows the agent to identify and avoid potentially dangerous actions through prediction.\n",
        "\n",
        "-   **Disadvantages:**\n",
        "    *   **Model Bias:** If the learned model is not accurate, planning based on that model will lead to suboptimal actions.\n",
        "    *   **Computational Cost:** Planning, especially for large state spaces, can be computationally expensive.\n",
        "\n",
        "**Model-Free Reinforcement Learning: The Pragmatist**\n",
        "\n",
        "-   **Definition:** Model-free agents don't learn or use an explicit environment model. Instead, they directly learn a *policy* (a mapping from states to actions) or a *value function* (estimating the value of states or state-action pairs) through trial and error.\n",
        "-   **Novel Analog:** **A Chef Learning a Recipe Through Taste**\n",
        "    -   **No Model:** The chef does not have a precise, step-by-step formula or a deep understanding of the chemical reactions of cooking.\n",
        "    -   **Trial and Error:** The chef experiments with different ingredients and techniques (actions), tasting the result (reward) and adjust his process (policy) based on the taste to achieve a good dish. They learn directly from experience and the end result (reward)\n",
        "    -   Just like the chef doesn't need to learn how the oven functions to make cookies, the agent can learn via trial and error without needing a specific transition model.\n",
        "\n",
        "-   **Mathematical Framework:**\n",
        "    1.  **Q-Learning (a common model-free approach):**\n",
        "        -   Learns an action-value function `Q(s, a)`, representing the expected cumulative reward for taking action `a` in state `s` and following the optimal policy thereafter.\n",
        "        -   The update rule is based on the Bellman equation and trial and error:\n",
        "            `Q(s_t, a_t)  â†  Q(s_t, a_t) + Î± * [r_t + Î³ * max_a Q(s_{t+1}, a) - Q(s_t, a_t)]`\n",
        "\n",
        "            Where:\n",
        "            -  `Q(s_t, a_t)` is the Q-value for taking action `a` at state `s`.\n",
        "            -  `Î±` is the learning rate (how much the current value changes).\n",
        "            -  `r_t` is the immediate reward at time `t`.\n",
        "            -  `Î³` is the discount factor.\n",
        "            -  `max_a Q(s_{t+1}, a)` is the maximum Q-value of all actions available at the next state, s_{t+1}.\n",
        "    2.  **SARSA (another common approach):**\n",
        "          Similar to Q-learning but uses the action the agent took and is based on policy rather than optimal policy for the update process.\n",
        "           `Q(s_t, a_t)  â†  Q(s_t, a_t) + Î± * [r_t + Î³ * Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]`\n",
        "           Where:\n",
        "           - `a_{t+1}` is the action the agent actually took based on a policy like epsilon-greedy from state s_{t+1}.\n",
        "    3.  **Policy Gradient Methods:** These methods directly learn the policy function, often represented as a neural network and updated using the gradient of the expected reward with respect to the policy's parameters.\n",
        "\n",
        "-   **Procedures:**\n",
        "    1.  **Initialization:** Initialize the Q-table or policy (usually randomly).\n",
        "    2.  **Action Selection:** Choose actions based on exploration-exploitation strategies like epsilon-greedy policies.\n",
        "    3.  **Interaction:** Interact with the environment and obtain `(s_t, a_t, r_t, s_{t+1})`.\n",
        "    4.  **Update:** Update the Q-function or policy parameters using the update rules.\n",
        "    5.  **Repeat:** Repeat steps 2-4 until the policy converges.\n",
        "\n",
        "-   **Advantages:**\n",
        "    *   **Simplicity:** Easier to implement as the agent doesn't need to maintain a model.\n",
        "    *   **Scalability:** Works well in high-dimensional and complex environments.\n",
        "    *   **No Model Bias:** Not limited by the accuracy of a potentially flawed model.\n",
        "\n",
        "-   **Disadvantages:**\n",
        "    *   **Sample Inefficiency:** Often requires a lot of interactions to learn an optimal policy.\n",
        "    *   **Limited Generalization:** May not generalize as well to new states, especially in cases with limited experience.\n",
        "    *   **Less Safe:** Can learn an optimal policy, but can be difficult to know if the actions taken are the most safe.\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| Feature         | Model-Based                                  | Model-Free                                   |\n",
        "| --------------- | -------------------------------------------- | --------------------------------------------- |\n",
        "| Environment Model | Explicitly learns or uses a model             | Does not learn or use a model                  |\n",
        "| Learning Goal    | Learns a model then plans using it    | Directly learns policy or value function       |\n",
        "| Sample Efficiency| High                                         | Low                                           |\n",
        "| Generalization  | Good                                         | Limited                                      |\n",
        "| Complexity      | More complex (modeling, planning)        | Less complex (direct learning)           |\n",
        "| Computational Cost| Higher (especially planning)                | Lower                                        |\n",
        "| Model Bias      | Susceptible to inaccuracies in model  | No direct model bias                 |\n",
        "| Safety          | Can plan and mitigate dangers                  | Can learn unsafe or suboptimal behaviours        |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EncwNzvioRx9"
      },
      "source": [
        "# Lecture 1\n",
        "\n",
        "- **Return** is the sum of future rewards:\n",
        "- *   $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$\n",
        "- **Value** is the expected cumulative reward from a state *s*:\n",
        "- *   $v(s) = E[G_t | S_t = s]$ - *   $ = E[R_{t+1} + R_{t+2} + R_{t+3} + ... | S_t = s]$\n",
        "- **Returns and values** can be defined **recursively**:\n",
        "- *   $G_t = R_{t+1} + G_{t+1}$ - *   $v(s) = E[R_{t+1} + v(S_{t+1}) | S_t = s]$\n",
        "- A **mapping from states to actions** is called a **policy**.\n",
        "- It is also possible to **condition the value on actions:**\n",
        "- *   $q(s, a) = E[G_t | S_t = s, A_t = a]$- *   $ = E[R_{t+1} + R_{t+2} + R_{t+3} + ... | S_t = s, A_t = a]$\n",
        "- The **actual value function** is the **expected return**:\n",
        "- *   $v^\\pi(s) = E[G_t | S_t = s, \\pi]$ - *   $= E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, \\pi]$\n",
        "- The **value function** also has a **recursive form** because the return has a recursive form:\n",
        "\n",
        "    *   $v^\\pi(s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t \\sim \\pi (s)]$\n",
        "    *   $= E[R_{t+1} + \\gamma v^\\pi(S_{t+1}) | S_t = s, A_t \\sim \\pi (s)]$\n",
        "    *   where $a \\sim \\pi(s)$ means *a* is chosen by policy $\\pi$ in state *s*\n",
        "-   A **similar equation** holds for the **optimal value**:\n",
        "\n",
        "    * $$v^*(s) = \\max\\limits_{a} E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t = s, \\\\ A_t = a]$$\n",
        "        *   This does not depend on a policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjtn0UIzNoAF"
      },
      "source": [
        "# Lecture 2\n",
        "\n",
        "### **Values and Regret**\n",
        "\n",
        "*   The **action-value** for an action *a* is the expected reward:\n",
        "    $q(a) = E[R_t | A_t = a]$\n",
        "*   The **optimal value** is:\n",
        "    $v^* = max_{a âˆˆ A} q(a) = max_a E[R_t | A_t = a]$\n",
        "*   **Regret** of an action *a* is:\n",
        "    $Î”_a = v^* - q(a)$\n",
        "*   The **total regret** is:\n",
        "    $L_t = \\sum_{n=1}^{t} v^* - q(A_n) = \\sum_{n=1}^{t} Î”_{A_n}$\n",
        "\n",
        "### **Action Values**\n",
        "\n",
        "*   A simple estimate of the action value is the average of the sampled rewards:\n",
        "$$Q_t(a) = \\frac{\\sum_{n=1}^{t} I(A_n = a) R_n}{\\sum_{n=1}^{t} I(A_n = a)}$$\n",
        "    where\n",
        "    -  I(Â·) is the indicator function:  I(True) 1 and I(False) = 0\n",
        "    -  The count for action *a* is:\n",
        "        $N_t(a) = \\sum_{n=1}^{t} I(A_n = a)$\n",
        "*   This can also be updated incrementally:\n",
        "$$ Q_t(A_t) = Q_{t-1}(A_t) + Î±_t (R_t - Q_{t-1}(A_t))âˆ€_a, A_t: Q_t(a) = Q_{t-1}(a) $$\n",
        "\n",
        "  $$ Î±_t = \\frac{1}{N_t(A_t)}$ and $N_t(A_t) = N_{t-1}(A_t) + 1 $$\n",
        "  where $N_0(a) = 0$.\n",
        "\n",
        "### **Policy Search**\n",
        "\n",
        "*   Define **action preferences** $H_t(a)$ and a **policy**:\n",
        "$\\pi(a) = \\frac{e^{H_t(a)}}{\\sum_b e^{H_t(b)}}$\n",
        "* Update **policy parameters** such that the **expected value increases**:\n",
        "  $Î¸_{t+1} = Î¸_t + Î±âˆ‡_Î¸E[R_t | \\pi_{Î¸_t}]$\n",
        "    where $Î¸_t$ are the current policy parameters\n",
        "\n",
        "### **Gradient Bandits**\n",
        "\n",
        "*   **Log-likelihood trick**:\n",
        "    $âˆ‡_Î¸E[R_t|\\pi_Î¸] = âˆ‡_Î¸\\sum_a \\pi_Î¸(a)q(a)$\n",
        "\n",
        "    $= \\sum_a q(a)âˆ‡_Î¸\\pi_Î¸(a)$\n",
        "\n",
        "    $= \\sum_a q(a)\\pi_Î¸(a) \\frac{âˆ‡_Î¸\\pi_Î¸(a)}{\\pi_Î¸(a)}$\n",
        "\n",
        "    $= \\sum_a \\pi_Î¸(a)q(a) \\frac{âˆ‡_Î¸\\pi_Î¸(a)}{\\pi_Î¸(a)}$\n",
        "\n",
        "    $= E[R_t \\frac{âˆ‡_Î¸\\pi_Î¸(A_t)}{\\pi_Î¸(A_t)}] = E[R_t âˆ‡_Î¸log\\pi_Î¸(A_t)]$\n",
        "\n",
        "*   **Stochastic gradient ascent**:\n",
        "    $Î¸ = Î¸ + Î±R_t âˆ‡_Î¸log\\pi_Î¸(A_t)$\n",
        "*   For **softmax**:\n",
        "    $H_{t+1}(a) = H_t(a) + Î±R_t \\frac{âˆ‚log\\pi_t(A_t)}{âˆ‚H_t(a)}$\n",
        "\n",
        "    $= H_t(a) + Î±R_t(I(a = A_t) - \\pi_t(a))$\n",
        "\n",
        "    $â‡’ H_{t+1}(A_t) = H_t(A_t) + Î±R_t(1 - \\pi_t(A_t))$\n",
        "    \n",
        "    $H_{t+1}(a) = H_t(a) - Î±R_t\\pi_t(a)$ if $a â‰  A_t$\n",
        "\n",
        "### **How Well Can We Do?**\n",
        "\n",
        "*   **Theorem (Lai and Robbins)**:\n",
        "  $lim_{tâ†’âˆž} L_t â‰¥ log t \\sum_{a | Î”_a > 0} \\frac{Î”_a}{KL(R_a || R_{a^*})}$\n",
        "\n",
        "### **Counting Regret**\n",
        "\n",
        "*   **Total regret**:\n",
        "    $L_t = \\sum_{n=1}^{t} Î”_{A_n} = \\sum_{a âˆˆ A}N_t(a) Î”_a$\n",
        "\n",
        "### **Upper Confidence Bounds**\n",
        "\n",
        "*   **Hoeffding's Inequality**:\n",
        "    $P( \\bar{X}_n + u â‰¤ \\mu) â‰¤ e^{-2nu^2}$\n",
        "*   Applying **Hoeffding's Inequality** to bandits with bounded rewards:\n",
        "    $P(Q_t(a) + U_t(a) â‰¤ q(a)) â‰¤ e^{-2N_t(a)U_t(a)^2}$\n",
        "*   **Upper confidence bound**:\n",
        "    $U_t(a) = \\sqrt{\\frac{-log p}{2N_t(a)}}$\n",
        "*   Reduce *p* as we observe more rewards, e.g., $p = 1/t$:\n",
        "    $U_t(a) = \\sqrt{\\frac{log t}{2N_t(a)}}$\n",
        "*   **UCB algorithm**:\n",
        "    $a_t = argmax_{aâˆˆA} Q_t(a) + c\\sqrt{\\frac{log t}{N_t(a)}}$\n",
        "*   **Theorem (Auer et al., 2002)**:\n",
        "    $L_t â‰¤ 8\\sum_{a | Î”_a > 0}\\frac{log t}{Î”_a} + O(\\sum_a Î”_a), âˆ€_t$.\n",
        "\n",
        "### **Bayesian Bandits: Example**\n",
        "\n",
        "*   **Posterior is a Beta distribution**:\n",
        "    $Beta(x_a, y_a)$\n",
        "    with initial parameters $x_a = 1$ and $y_a = 1$ for each action *a*\n",
        "*   **Updating the posterior**:\n",
        "    $x_{A_t} â† x_{A_t} + 1$ when $R_t = 0$\n",
        "    $y_{A_t} â† y_{A_t} + 1$ when $R_t = 1$\n",
        "\n",
        "### **Probability Matching**\n",
        "\n",
        "*   Select action *a* according to the probability that *a* is optimal:\n",
        "    $\\pi_t(a) = P(q(a) = max_{a'} q(a') | H_{t-1})$\n",
        "\n",
        "### **Thompson Sampling**\n",
        "\n",
        "*   Sample $Q_t(a) \\sim p_t(q(a)), âˆ€_a$\n",
        "*   Select action maximizing sample:\n",
        "    $A_t = argmax_a Q_t(a)$\n",
        "\n",
        "### **Information State Space**\n",
        "\n",
        "*   **Information state**:\n",
        "    $I = (Î±, Î²)$\n",
        "\n",
        "### **Example: Bernoulli Bandits**\n",
        "\n",
        "*   **Bernoulli bandit**:\n",
        "    $P(R_t = 1 | A_t = a) = \\mu_a$\n",
        "    $P(R_t = 0 | A_t = a) = 1 - \\mu_a$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuFN3snMQRKD"
      },
      "source": [
        "# Lecture 3\n",
        "\n",
        "### **Markov Decision Process**\n",
        "\n",
        "*   A **Markov Decision Process (MDP)** is defined as a tuple $(S, A, p, \\gamma)$, where:\n",
        "    *   $S$ is the set of all possible states.\n",
        "    *   $A$ is the set of all possible actions.\n",
        "    *   $p(r, s' | s, a)$ is the joint probability of a reward *r* and next state *s'*, given a state *s* and action *a*.\n",
        "    *   $\\gamma \\in$ is a discount factor that trades off later rewards to earlier ones.\n",
        "\n",
        "*   It can also be defined as a tuple $(S, A, p, r, \\gamma)$, where:\n",
        "    *   $S$ is the set of all possible states.\n",
        "    *   $A$ is the set of all possible actions.\n",
        "    *   $p(s' | s, a)$ is the probability of transitioning to *s'*, given a state *s* and action *a*.\n",
        "    *   $r: S \\times A \\rightarrow R$ is the expected reward, achieved on a transition starting in (*s*, *a*):  $r = E[R | s, a]$.\n",
        "    *   $\\gamma \\in$ is a discount factor that trades off later rewards to earlier ones.\n",
        "\n",
        "*   The **dynamics** of the problem are defined by *p*.\n",
        "*   Marginalized state transitions:\n",
        "    $p(s' | s, a) = \\sum_r p(s', r | s, a)$\n",
        "*   Marginalized expected reward:\n",
        "    $E[R | s, a] = \\sum_r r\\sum_{s'}p(r, s' | s, a)$.\n",
        "\n",
        "### **Markov Property**\n",
        "\n",
        "*   A state *s* has the **Markov property** when for states $\\forall s' \\in S$:\n",
        "  $p(S_{t+1}=s' | S_t = s) = p(S_{t+1}=s' | h_{t-1}, S_t=s)$ for all possible histories $h_{t-1} = \\{S_1,...,S_{t-1},A_1,...,A_{t-1},R_1,...,R_{t-1}\\}$.\n",
        "\n",
        "### **Returns**\n",
        "\n",
        "*   **Undiscounted return** (episodic/finite horizon problem):\n",
        "  $G_t = R_{t+1} + R_{t+2} + ... + R_T = \\sum_{k=0}^{T-t-1} R_{t+k+1}$\n",
        "*   **Discounted return** (finite or infinite horizon problem):\n",
        "  $G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t}R_T = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$\n",
        "*   **Average return** (continuing, infinite horizon problem):\n",
        "  $G_t = \\frac{1}{T-t-1}(R_{t+1} + R_{t+2} + ... + R_T) = \\frac{1}{T-t-1}\\sum_{k=0}^{T-t-1} R_{t+k+1}$\n",
        "*   **Discounted returns** $G_t$ for infinite horizon $T \\rightarrow \\infty$:\n",
        "  $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
        "\n",
        "### **Policies**\n",
        "\n",
        "*   A **policy** $\\pi : S \\times A \\rightarrow$ maps every state *s* to the probability of taking action *a* $\\in A$ in state *s*, denoted by $\\pi(a | s)$.\n",
        "*   For **deterministic policies**, $a_t = \\pi(s_t)$ denotes the action taken by the policy.\n",
        "\n",
        "### **Value Functions**\n",
        "\n",
        "*   The **value function** $v(s)$ gives the long-term value of state *s*:\n",
        "    $v^\\pi(s) = E[G_t | S_t = s, \\pi]$\n",
        "*   **State-action values**:\n",
        "    $q^\\pi(s, a) = E[G_t | S_t = s, A_t = a, \\pi]$\n",
        "*   **Connection** between the value function and state-action values:\n",
        "    $v^\\pi(s) = \\sum_a \\pi(a | s) q^\\pi(s, a) = E[q^\\pi(S_t, A_t) | S_t = s, \\pi], \\forall s$\n",
        "\n",
        "### **Optimal Value Function**\n",
        "\n",
        "*   The **optimal state-value function** $v^*(s)$ is the maximum value function over all policies:\n",
        "  $v^*(s) = \\max_\\pi v^\\pi(s)$\n",
        "*   The **optimal action-value function** $q^*(s, a)$ is the maximum action-value function over all policies:\n",
        "  $q^*(s, a) = \\max_\\pi q^\\pi(s, a)$\n",
        "\n",
        "### **Optimal Policy**\n",
        "\n",
        "*   **Partial ordering over policies**:\n",
        "  $\\pi \\geq \\pi' \\iff v^\\pi(s) \\geq v^{\\pi'}(s), \\forall s$\n",
        "*   An **optimal policy** can be found by maximizing over $q^*(s, a)$:\n",
        "$$\\pi^*(s, a) = \\begin{cases}\n",
        "      1 & \\text{if $a = argmax_{a \\in A} q^*(s, a)$}\\\\\n",
        "      0 & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "### **Bellman Equations**\n",
        "\n",
        "*   The value function can be defined **recursively**:\n",
        "    $v^\\pi(s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s, \\pi]$\n",
        "    $ = E[R_{t+1} + \\gamma v^\\pi(S_{t+1}) | S_t = s, A_t \\sim \\pi(S_t)]$\n",
        "    $= \\sum_a \\pi(a | s) \\sum_r \\sum_{s'} p(r, s' | s, a)(r + \\gamma v^\\pi(s'))$\n",
        "\n",
        "\n",
        "*   **State-action values**:\n",
        "    $q^\\pi(s, a) = E[R_{t+1} + \\gamma v^\\pi(S_{t+1}) | S_t = s, A_t = a]$\n",
        "    $= E[R_{t+1} + \\gamma q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$\n",
        "    $= \\sum_r \\sum_{s'} p(r, s' | s, a)(r + \\gamma \\sum_{a'} \\pi(a' | s')q^\\pi(s', a'))$\n",
        "\n",
        "*   **Bellman Expectation Equation**:\n",
        "    $v^\\pi(s) = \\sum_a \\pi(s, a) [r(s, a) + \\gamma \\sum_{s'} p(s' | a, s) v^\\pi(s')]$\n",
        "    $q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s' | a, s) \\sum_{a' \\in A} \\pi(a' | s')q^\\pi(s', a')$\n",
        "\n",
        "*   **Bellman Optimality Equations**:\n",
        "    $v^*(s) = \\max_a [r(s, a) + \\gamma \\sum_{s'} p(s' | a, s) v^*(s')]$\n",
        "    $q^*(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s' | a, s) \\max_{a' \\in A} q^*(s', a')$\n",
        "\n",
        "### **Bellman Equation in Matrix Form**\n",
        "\n",
        "*   The Bellman value equation for a given $\\pi$ can be expressed using matrices:\n",
        "  $v = r^\\pi + \\gamma P^\\pi v$\n",
        "    $v_i = v(s_i)$\n",
        "\n",
        "    $r^\\pi_i = E[R_{t+1} | S_t = s_i, A_t \\sim \\pi(S_t)$\n",
        "\n",
        "    $$P^\\pi_{ij} = p(s_j | s_i) = \\sum_a \\pi(a | s_i) p(s_j | s_i, a)$$\n",
        "\n",
        "*   The Bellman equation can be solved directly:\n",
        "    $v = r^\\pi + \\gamma P^\\pi v$\n",
        "    $(I - \\gamma P^\\pi)v = r^\\pi$\n",
        "    $v = (I - \\gamma P^\\pi)^{-1}r^\\pi$\n",
        "\n",
        "### **Policy Evaluation**\n",
        "\n",
        "*   Iterative update:\n",
        "    $\\forall s: v_{k+1}(s) \\leftarrow E[R_{t+1} + \\gamma v_k(S_{t+1}) | s, \\pi]$\n",
        "\n",
        "### **Policy Improvement**\n",
        "\n",
        "*   Iterative update:\n",
        "    $\\forall s: \\pi_{new}(s) = argmax_a q^\\pi(s, a)$\n",
        "    $= argmax_a E[R_{t+1} + \\gamma v^\\pi(S_{t+1}) | S_t = s, A_t = a]$\n",
        "\n",
        "### **Value Iteration**\n",
        "\n",
        "*   Iterative update:\n",
        "    $\\forall s: v_{k+1}(s) \\leftarrow \\max_a E[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t = s, A_t = s]$\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fa8DpARqkc"
      },
      "source": [
        "# Lecture 4\n",
        "\n",
        "### **Preliminaries**\n",
        "\n",
        "*   A mapping $T: X\\rightarrow X$ is an **$\\alpha$-contraction mapping** if for any $x_1, x_2 \\in X$, $\\exists \\alpha \\in [0, 1)$ s.t.:\n",
        "  $$\\|Tx_1 - Tx_2\\| \\leq \\alpha \\|x_1 - x_2\\|$$\n",
        "*   If $\\alpha \\in$, then  *T*  is called **non-expanding**.\n",
        "\n",
        "### **Bellman Expectation Equations**\n",
        "\n",
        "Given an MDP,  $M = \\langle S, A, p, r, \\gamma \\rangle$, for any policy $\\pi$, the value functions obey the following expectation equations:\n",
        "\n",
        "*   **State-value function**:\n",
        "    $$v^\\pi(s) = \\sum_a \\pi(s, a)[r(s, a) + \\gamma \\sum_{s'}p(s'|a,s)v^\\pi(s')]$$\n",
        "*   **Action-value function**:\n",
        "    $$q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s'|a, s)\\sum_{a' \\in A}\\pi(a'|s')q^\\pi(s', a')$$\n",
        "\n",
        "### **Bellman Optimality Equations**\n",
        "\n",
        "Given an MDP, $M = \\langle S, A, p, r, \\gamma \\rangle$, the optimal value functions obey the following expectation equations:\n",
        "\n",
        "*   **Optimal state-value function**:\n",
        "  $$v^*(s) = \\max_a [r(s, a) + \\gamma \\sum_{s'} p(s'|a, s)v^*(s')]$$\n",
        "*   **Optimal action-value function**:\n",
        "  $$q^*(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s'|a, s) \\max_{a' \\in A} q^*(s', a')$$\n",
        "\n",
        "### **Bellman Optimality Operator**\n",
        "\n",
        "Given an MDP, $M = \\langle S, A, p, r, \\gamma \\rangle$, let $V \\equiv V_S$ be the space of bounded real-valued functions over $S$.\n",
        "The **Bellman Optimality Operator**\n",
        " $$T^*_V: V \\rightarrow V$$\n",
        " is defined point-wise as:\n",
        "\n",
        "$$(T^*_V f)(s) = \\max_a [r(s, a) + \\gamma \\sum_{s'} p(s'|a, s)f(s')], \\forall f \\in V$$\n",
        "\n",
        "As a common convention, the index $V$ is dropped, so $T^* = T^*_V$.\n",
        "\n",
        "### **Properties of the Bellman Operator** $T^*$\n",
        "\n",
        "*   It has one unique fixed point $v^*$:\n",
        "$$T^* v^* = v^*$$\n",
        "\n",
        "*   $T^*$ is a $\\gamma$-contraction w.r.t.  $\\|\\cdot\\|_\\infty$:\n",
        "$$\\|T^*v - T^*u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty, \\forall u, v \\in V$$\n",
        "\n",
        "*   $T^*$ is monotonic:\n",
        "$$\\forall u, v \\in V \\text{ s.t. } u \\leq v \\text{, component-wise, then } T^* u \\leq T^* v$$\n",
        "\n",
        "### **Proof that** $T^*$ **is a** $\\gamma$**-contraction w.r.t.** $\\|\\cdot\\|_\\infty$\n",
        "\n",
        "$$|T^*v(s)-T^*u(s)| = |\\max_a[r(s, a) + \\gamma E_{s'|s,a}v(s')] - \\max_b[r(s, b) + \\gamma E_{s''|s, b}u(s'')]|$$\n",
        "\n",
        "$$\\leq \\max_a |[r(s, a) + \\gamma E_{s'|s, a}v(s')] - [r(s, a) + \\gamma E_{s'|s, a}u(s')]|$$\n",
        "\n",
        "$$= \\gamma \\max_a |E_{s'|s, a}[v(s') - u(s')]|$$\n",
        "\n",
        "$$\\leq \\gamma \\max_{s'}|[v(s') - u(s')]|$$\n",
        "\n",
        "Thus, we get:\n",
        "\n",
        "$$\\|T^* v - T^* u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty, \\forall u, v \\in V$$\n",
        "\n",
        "**Note**: Step (6)-(7) uses: $|\\max_a f(a) - \\max_b g(b)| \\leq \\max_a|f(a) - g(a)|$\n",
        "\n",
        "### **Proof that** $T^*$ **is monotonic**\n",
        "\n",
        "Given $v(s) \\leq u(s), \\forall s \\implies r(s, a) + E_{s'|s, a} u(s') \\leq r(s, a) + E_{s'|s,a}v(s')$\n",
        "\n",
        "$$T^*v(s) - T^*u(s) = \\max_a [r(s, a) + \\gamma E_{s'|s, a} v(s')] - \\max_b [r(s, b) + \\gamma E_{s''|s, b} u(s'')]$$\n",
        "\n",
        "$$\\leq \\max_a ([r(s, a) + \\gamma E_{s'|s, a} v(s')] - [r(s, a) + \\gamma E_{s'|s, a} u(s')])$$\n",
        "\n",
        "$$\\leq 0, \\forall s$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$T^*v(s) \\leq T^* u(s), \\forall s \\in S$$\n",
        "\n",
        "### **Value Iteration through the Lens of the Bellman Operator**\n",
        "\n",
        "**Value Iteration**\n",
        "\n",
        "*   Start with $v_0$.\n",
        "*   Update values: $v_{k+1} = T^* v_k$.\n",
        "\n",
        "As $k \\rightarrow \\infty, v_k \\rightarrow^{\\|\\cdot\\|_\\infty} v^*$.\n",
        "\n",
        "**Proof:** Direct application of the Banach Fixed Point Theorem.\n",
        "\n",
        "$$\\|v_k - v^*\\|_\\infty = \\|T^* v_{k-1} - v^*\\|_\\infty = \\|T^* v_{k-1} - T^* v^*\\|_\\infty \\text{ (fixed point property)}$$\n",
        "\n",
        "$$\\leq \\gamma \\|v_{k-1} - v^*\\|_\\infty \\text{ (contraction property)}$$\n",
        "\n",
        "$$\\leq \\gamma^k \\|v_0 - v^*\\|_\\infty \\text{ (iterative application)}$$\n",
        "\n",
        "### **Bellman Expectation Operator**\n",
        "\n",
        "Given an MDP, $M = \\langle S, A, p, r, \\gamma \\rangle$, let $V \\equiv V_S$ be the space of bounded real-valued functions over *S*. For any policy $\\pi: S \\times A \\rightarrow$,\n",
        "the **Bellman Expectation Operator**\n",
        "$T^\\pi_V: V \\rightarrow V$ is defined point-wise as:\n",
        "\n",
        "$$(T^\\pi_V f)(s) = \\sum_a \\pi(s, a)[r(s, a) + \\gamma \\sum_{s'} p(s'|a, s)f(s')], \\forall f \\in V$$\n",
        "\n",
        "### **Properties of the Bellman Operator** $T^\\pi$\n",
        "\n",
        "*   It has one unique fixed point $v^\\pi$:\n",
        "$$T^\\pi v^\\pi = v^\\pi$$\n",
        "*   $T^\\pi$ is a $\\gamma$-contraction w.r.t. $\\|\\cdot\\|_\\infty$:\n",
        "$$\\|T^\\pi v - T^\\pi u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty, \\forall u, v \\in V$$\n",
        "*   $T^\\pi$ is monotonic:\n",
        "$$\\forall u, v \\in V \\text{ s.t. } u \\leq v \\text{, component-wise, then } T^\\pi u \\leq T^\\pi v$$\n",
        "\n",
        "### **Proof that** $T^\\pi$ **is a** $\\gamma$**-contraction w.r.t.** $\\|\\cdot\\|_\\infty$\n",
        "\n",
        "$$T^\\pi v(s) - T^\\pi u(s) = \\sum_a \\pi(a|s)[r(s, a) + \\gamma E_{s'|s, a} v(s') - r(s, a) - \\gamma E_{s'|s, a}u(s')]$$\n",
        "\n",
        "$$= \\gamma \\sum_a \\pi(a|s) E_{s'|s, a}[v(s') - u(s')]$$\n",
        "\n",
        "\n",
        "$$\\implies |T^\\pi v(s) - T^\\pi u(s)| \\leq \\gamma \\max_ |\\\\{s'} |[v(s') - u(s')]|$$\n",
        "\n",
        "Thus, we get:\n",
        "\n",
        "$$\\|T^\\pi v - T^\\pi u\\|_\\infty \\leq \\gamma \\|v - u\\|_\\infty, \\forall u, v \\in V$$\n",
        "\n",
        "**Note**: Equation (14) also gives the monotonicity of $T^\\pi$.\n",
        "\n",
        "### **Policy Evaluation**\n",
        "\n",
        "**(Iterative) Policy Evaluation**\n",
        "\n",
        "*   Start with $v_0$.\n",
        "*   Update values: $v_{k+1} = T^\\pi v_k$.\n",
        "\n",
        "As $k \\rightarrow \\infty, v_k \\rightarrow^{\\|\\cdot\\|_\\infty} v^\\pi$.\n",
        "\n",
        "**Proof**: Direct application of the Banach Fixed Point Theorem.\n",
        "\n",
        "### **Dynamic Programming with Bellman Operators**\n",
        "\n",
        "**Value Iteration**\n",
        "\n",
        "*   Start with $v_0$.\n",
        "*   Update values: $v_{k+1} = T^* v_k$.\n",
        "\n",
        "**Policy Iteration**\n",
        "\n",
        "*   Start with $\\pi_0$.\n",
        "\n",
        "*   Iterate:\n",
        "\n",
        "    *   Policy Evaluation: $v^{\\pi_i}$ (e.g., for instance, by iterating $T^\\pi: v_k = T^{\\pi_i} v_{k-1} \\implies v_k \\rightarrow v^{\\pi_i} \\text{ as } k \\rightarrow \\infty$)\n",
        "    *   Greedy Improvement: $\\pi_{i+1} = \\argmax_a q^{\\pi_i}(s, a)$\n",
        "\n",
        "Similarly for $q^\\pi: S \\times A \\rightarrow R$ functions:\n",
        "\n",
        "Given an MDP, $M = \\langle S, A, p, r, \\gamma \\rangle$, let $Q \\equiv Q_{S,A}$ be the space of bounded real-valued functions over $S \\times A$. For any policy $\\pi: S \\times A \\rightarrow$, the **Bellman Expectation Operator** $T^\\pi_Q: Q \\rightarrow Q$ is defined point-wise as:\n",
        "\n",
        "$$(T^\\pi_Q f)(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s'|a, s) \\sum_{a' \\in A} \\pi(a'|s')f(s', a'), \\forall f \\in Q$$\n",
        "\n",
        "This operator has a unique fixed point which corresponds to the action-value function $q^\\pi$ in our MDP *M*. It has the same properties as $T^\\pi$:  $\\gamma$-contraction and monotonicity.\n",
        "\n",
        "Similarly for $q^*: S \\times A \\rightarrow R$ functions:\n",
        "\n",
        "Given an MDP, $M = \\langle S, A, p, r, \\gamma \\rangle$, let $Q \\equiv Q_{S, A}$ be the space of bounded real-valued functions over $S \\times A$. The **Bellman Optimality Operator** $T^*_Q: Q \\rightarrow Q$ is defined as:\n",
        "\n",
        "$$(T^*_Q f)(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s'|a, s) \\max_{a' \\in A} f(s', a'), \\forall f \\in Q$$\n",
        "\n",
        "This operator has a unique fixed point which corresponds to the action-value function $q^*$ in our MDP *M*. It has the same properties as $T^*$:  $\\gamma$-contraction and monotonicity.\n",
        "\n",
        "### **Approximate Value Iteration**\n",
        "\n",
        "**Approximate Value Iteration**\n",
        "\n",
        "*   Start with $v_0$.\n",
        "*   Update values: $v_{k+1} = A T^* v_k (v_{k+1} \\approx T^* v_k)$.\n",
        "\n",
        "**Question**: As $k \\rightarrow \\infty$, does $v_k \\rightarrow^{\\|\\cdot\\|_\\infty} v^*$?\n",
        "\n",
        "**Answer**: In general, no.\n",
        "\n",
        "### **ADP: Approximating the Value Function**\n",
        "\n",
        "Using a function approximator $v_\\theta(s)$ with a parameter vector $\\theta \\in R^m$:\n",
        "\n",
        "*   The estimated value function at iteration *k* is $v_k = v_{\\theta_k}$\n",
        "\n",
        "*   Use dynamic programming to compute $v_{\\theta_{k+1}}$ from $v_{\\theta_k}$\n",
        "\n",
        "$$T^* v_k(s) = \\max_a E[R_{t+1} + \\gamma v_k(S_{t+1})|S_t = s]$$\n",
        "\n",
        "*   Fit $\\theta_{k+1}$ s.t. $v_{\\theta_{k+1}} \\approx T^*v_k(s)$\n",
        "\n",
        "*   For instance, with respect to a squared loss over the state-space:\n",
        "\n",
        "$$\\theta_{k+1} = \\argmin_{\\theta_{k+1}} \\sum_s (v_{\\theta_{k+1}}(s) - T^*v_k(s))^2$$\n",
        "\n",
        "### **Example of Divergence with Dynamic Programming**\n",
        "\n",
        "$$\\theta_{k+1} = \\argmin_\\theta \\sum_{s \\in S}(v_\\theta(s) - E[v_{\\theta_k}(S_{t+1})|S_t=s])^2$$\n",
        "\n",
        "$$= \\argmin_\\theta (\\theta - \\gamma 2 \\theta_k)^2 + (2\\theta - \\gamma(1 - \\epsilon)2\\theta_k)^2$$\n",
        "\n",
        "$$= \\frac{2(3-2\\epsilon)\\gamma}{5}\\theta_k$$\n",
        "\n",
        "### **Performance of a Greedy Policy**\n",
        "\n",
        "Consider an MDP. Let $q: S \\times A \\rightarrow R$ be an arbitrary function and let $\\pi$ be the greedy policy associated with *q*, then:\n",
        "\n",
        "$$\\|q^* - q^\\pi\\|_\\infty \\leq \\frac{2 \\gamma}{1 - \\gamma}\\|q^* - q\\|_\\infty$$\n",
        "\n",
        "where $q^*$ is the optimal value function associated with this MDP.\n",
        "\n",
        "### **Proof of Performance of a Greedy Policy**\n",
        "\n",
        "**Statement**:\n",
        "\n",
        "$$\\|q^* - q^\\pi\\|_\\infty \\leq \\frac{2\\gamma}{1 - \\gamma}\\|q^* - q\\|_\\infty$$\n",
        "\n",
        "**Proof**:\n",
        "\n",
        "$$\\|q^* - q^\\pi\\|_\\infty = \\|q^* - T^\\pi q + T^\\pi q - q^\\pi\\|_\\infty$$\n",
        "\n",
        "$$\\leq \\|q^* - T^\\pi q\\|_\\infty + \\|T^\\pi q - q^\\pi\\|_\\infty$$\n",
        "\n",
        "$$= \\|T^* q^* - T^*q\\|_\\infty + \\|T^\\pi q - T^\\pi q^\\pi\\|_\\infty$$\n",
        "\n",
        "$$\\leq \\gamma \\|q^* - q\\|_\\infty + \\gamma \\underbrace{\\|q - q^\\pi\\|_\\infty}_{\\leq \\|q - q^*\\|_\\infty + \\|q^* - q^\\pi\\|_\\infty}$$\n",
        "\n",
        "$$\\leq 2\\gamma \\|q^* - q\\|_\\infty + \\gamma \\|q^* - q^\\pi\\|_\\infty$$\n",
        "\n",
        "Re-arranging: $(1 - \\gamma)\\|q^* - q^\\pi\\|_\\infty \\leq 2\\gamma\\|q^* - q\\|_\\infty$.\n",
        "\n",
        "### **Approximate Policy Iteration**\n",
        "\n",
        "**Approximate Policy Iteration**\n",
        "\n",
        "*   Start with $\\pi_0$.\n",
        "*   Iterate:\n",
        "    *   Policy Evaluation: $q_i = Aq^{\\pi_i} (q_i \\approx q^{\\pi_i})$\n",
        "    *   Greedy Improvement: $\\pi_{i+1} = \\argmax_a q_i(s, a)$\n",
        "\n",
        "**Question 1**: As $i \\rightarrow \\infty$, does $q_i \\rightarrow^{\\|\\cdot\\|_\\infty} q^*$?\n",
        "\n",
        "**Answer**: In general, no.\n",
        "\n",
        "**Question 2**: Or, does $\\pi_i$ converge to the optimal policy?\n",
        "\n",
        "**Answer**: In general, no.\n",
        "\n",
        "### **Approximate Dynamic Programming**\n",
        "\n",
        "**Approximate Value Iteration**\n",
        "\n",
        "*   Start with $v_0$.\n",
        "*   Update values: $v_{k+1} = AT^*v_k (v_{k+1} \\approx T^* v_k)$.\n",
        "\n",
        "**Approximate Policy Iteration**\n",
        "\n",
        "*   Start with $\\pi_0$.\n",
        "*   Iterate:\n",
        "    *   Policy Evaluation: $q_i = Aq^{\\pi_i} (q_i \\approx q^{\\pi_i})$\n",
        "    *   Greedy Improvement: $\\pi_{i+1} = \\argmax_a q_i(s, a)$\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr37a9IdRqgu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU8xbmpMoG_h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
